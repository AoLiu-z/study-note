# 大语言模型学习笔记（LLMs Learning note）
# 一、Transformer
1. **词嵌入 Embedding、位置编码 Encoding**
- 文本数据-->通过Embedding得到词向量
- 词向量 + 位置编码获得原始数据X
2. **自注意力机制 Self-Attention**
- 原始数据X乘上对应的$W_q,W_k,W_v$参数矩阵，获得Q,K,V
- 计算注意力分数，通过点积计算Q、K之间的相似度：score = $Q*K^T$，为防止结果过大，对分数进行缩放(除以$\sqrt d_k)，其中 d_k$是矩阵K的维度
- 然后对分数进行Softmax操作，将分数进行归一化处理，得到注意力权重
- 最后用注意力权重对V进行加权求和，就得到了编码器的输出结果
3. **多头注意力机制 Multi-head Attention**（以h头为例）
- 原始数据X乘上h对的（$W_q,W_k,W_v$），得到相对应的Q,K,V矩阵，然后各自进行自注意力机制计算，最终获得$Z_1,...,Z_h$。然后将结果进行拼接，得到矩阵$Z_0$，再乘以一个权重矩阵 $W_0$ 便得到最终结果
4. **残差网络和层标准化（Layer Norm）**
- 将结果带入到残差网络并进行层标准化
- 每个自注意力模块、多头注意力模块、前馈神经网络 FNN 模块后面都存在一层 残差网络和层标准化
- 残差网络的目的是：解决梯度消失和梯度爆炸的问题
- 层标准化的目的是：加速神经网络训练过程并取得更好的泛化性能
5. **编码器 Encoder**
- 由多头注意力模块、残差层标准化、前馈FNN、残差层标准化组成
- Transformer 通常由多个相同的编码器和解码器层堆叠而成
6. **解码器 Decoder**
- 由掩码多头注意力模块（Masked MHA）、残差层标准化、Encoder-Decoder-Attention模块、残差层标准化、FNN、残差层标准化组成
- 掩码 Mask分类：Padding Mask 、 Sequence Mask
- Padding Mask 因为每个批次输入序列的长度不一样，故需要对输入序列进行对其。对于短的序列后面填充0，长的序列则要截断。之后，将这些位置的值加上一个非常大的负数，经过softmax后这些位置的概率就会接近0。
- Sequence Mask 是为了使得Decoder 不能看见未来的信息，解码器输出只依赖当前时刻之前的输出，所以需要将t之后的信息隐藏起来。产生一个上三角矩阵，上三角的值全为0，把这个矩阵作用到每个序列上，就可以掩盖未来的信息。
7. **Encoder-Decoder-Attention模块**

- 与self attention不同的是，这个模块的K,V来自于Encoder的输出，Q来自于前一层的输出构造Query矩阵。

# 二、大模型基础
1. Top-k采样
- 选取前k个大概率的样本作为采样样本
2. Top-p采样
- 选取概率 $\ge p$ 作为采样样本
3. Temperature 温度
- 随机性控制参数，靠近0则随机性低，靠近1则随机性高

## Transformer 三种架构
### 1.Encoder-only
1.1 代表模型---**BERT、RoBERTa、ALBERT（模型与训练数据逐渐变大）**
- 可完整注意上下文，捕捉上下文之间的语义和依赖关系，适用于“判别任务”，不适用于生成任务。
- 模型由输入文本、分词器、词嵌入矩阵、位置编码、若干个编码模块、输出头组成
- 输出头根据任务的不同，设置不同的输出头
### 2.Encoder-Decoder
- 在编码器后加入解码器，可以通过上下文来生成序列，适用于“判别任务”和“生成任务”，但训练和推理成本增加。
- 通过交叉注意力机制来实现编码器与解码器之间的交互。（即编码器K,V与解码器Q进行注意力计算）
### 3.Decoder-only
3.1 代表模型---**GPT系列、LLaMA系列**
- 只关注上文，下文被掩盖，适用于“生成任务”。
## Prompt 
1. 基本元素：任务说明、问题、上下文、输出格式（如JSON格式）
2. prompt engineering 提示工程 ---优化prompt
3. prompt 工程技术：上下文学习、思维链 CoT
### 1. 上下文学习
1.1 零样本提示 zero-shot prompts
- 没有给大模型示例、样本
1.2 少样本提示 few-shot prompts
- 提供少量任务完成的示例、样本
1.3 缺点
- 缺乏逻辑推理能力，依照提供示例也不能很好的进行逻辑推理
### 2. 思维链 CoT  (Chain of Thoughts)
2.1 标准CoT: 给出示例，按照逻辑一步一步进行计算，标注成本高，算法成本低

2.2 Zero-shot CoT: prompt给出 请一步一步进行计算，标注成本低，算法成本高

2.3 Auto-shot CoT（大模型自动找到相似样本，生成示例，根据示例生成答案）
- 使用k-means算法，找到与用户提问相似的问题样本
- 利用zero-shot CoT 生成思维链内容，作为示例
- 以示例为少样本示例，引导大模型生成答案
### 3. 思维树 ToT (Tree of Thoughts)
- 处理大规模或复杂任务时，将问题任务分解为一系列子问题或子任务，这些子问题或子任务进一步细分，形成树状结构。
### 4. Self-Consistency（自身一致性）
- 引入多样性推理路径，选取频率最高（最一致）的答案，从而提高了模型推理的准确性

**步骤：**

1. 推理路径生成：在随机采样策略下，使用CoT或Zero-shot CoT来生成不同的答案
2. 汇总答案
3. 选择频率最高的作为答案
### 5. Universal Self-Consistency
- 利用大模型来选择最一致的答案